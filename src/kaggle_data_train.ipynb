{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lx0</th>\n",
       "      <th>lx1</th>\n",
       "      <th>lx2</th>\n",
       "      <th>lx3</th>\n",
       "      <th>lx4</th>\n",
       "      <th>lx5</th>\n",
       "      <th>lx6</th>\n",
       "      <th>lx7</th>\n",
       "      <th>lx8</th>\n",
       "      <th>...</th>\n",
       "      <th>ry2038</th>\n",
       "      <th>ry2039</th>\n",
       "      <th>ry2040</th>\n",
       "      <th>ry2041</th>\n",
       "      <th>ry2042</th>\n",
       "      <th>ry2043</th>\n",
       "      <th>ry2044</th>\n",
       "      <th>ry2045</th>\n",
       "      <th>ry2046</th>\n",
       "      <th>ry2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>74</td>\n",
       "      <td>122</td>\n",
       "      <td>127</td>\n",
       "      <td>122</td>\n",
       "      <td>119</td>\n",
       "      <td>115</td>\n",
       "      <td>112</td>\n",
       "      <td>106</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>646</td>\n",
       "      <td>634</td>\n",
       "      <td>630</td>\n",
       "      <td>622</td>\n",
       "      <td>617</td>\n",
       "      <td>615</td>\n",
       "      <td>611</td>\n",
       "      <td>607</td>\n",
       "      <td>605</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>-14</td>\n",
       "      <td>-4</td>\n",
       "      <td>14</td>\n",
       "      <td>-3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>383</td>\n",
       "      <td>392</td>\n",
       "      <td>383</td>\n",
       "      <td>386</td>\n",
       "      <td>389</td>\n",
       "      <td>381</td>\n",
       "      <td>388</td>\n",
       "      <td>384</td>\n",
       "      <td>385</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>201</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>197</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>114</td>\n",
       "      <td>168</td>\n",
       "      <td>167</td>\n",
       "      <td>171</td>\n",
       "      <td>166</td>\n",
       "      <td>172</td>\n",
       "      <td>166</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>194</td>\n",
       "      <td>197</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>-13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>-365</td>\n",
       "      <td>-368</td>\n",
       "      <td>-369</td>\n",
       "      <td>-369</td>\n",
       "      <td>-366</td>\n",
       "      <td>-372</td>\n",
       "      <td>-371</td>\n",
       "      <td>-371</td>\n",
       "      <td>-372</td>\n",
       "      <td>-374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>61</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>118</td>\n",
       "      <td>122</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>114</td>\n",
       "      <td>119</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>21</td>\n",
       "      <td>80</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>110</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>105</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-89</td>\n",
       "      <td>-88</td>\n",
       "      <td>-85</td>\n",
       "      <td>-83</td>\n",
       "      <td>-84</td>\n",
       "      <td>-86</td>\n",
       "      <td>-88</td>\n",
       "      <td>-90</td>\n",
       "      <td>-88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>76</td>\n",
       "      <td>63</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>12</td>\n",
       "      <td>-62</td>\n",
       "      <td>-58</td>\n",
       "      <td>-59</td>\n",
       "      <td>-57</td>\n",
       "      <td>-57</td>\n",
       "      <td>-57</td>\n",
       "      <td>-59</td>\n",
       "      <td>-55</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>193</td>\n",
       "      <td>194</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>189</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2</td>\n",
       "      <td>-77</td>\n",
       "      <td>-73</td>\n",
       "      <td>-70</td>\n",
       "      <td>-68</td>\n",
       "      <td>-71</td>\n",
       "      <td>-71</td>\n",
       "      <td>-73</td>\n",
       "      <td>-72</td>\n",
       "      <td>-75</td>\n",
       "      <td>...</td>\n",
       "      <td>-387</td>\n",
       "      <td>-381</td>\n",
       "      <td>-380</td>\n",
       "      <td>-378</td>\n",
       "      <td>-380</td>\n",
       "      <td>-380</td>\n",
       "      <td>-380</td>\n",
       "      <td>-381</td>\n",
       "      <td>-382</td>\n",
       "      <td>-383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>652 rows Ã— 8193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  lx0  lx1  lx2  lx3  lx4  lx5  lx6  lx7  lx8  ...  ry2038  ry2039  \\\n",
       "0       25   74  122  127  122  119  115  112  106   99  ...     646     634   \n",
       "1       31  -14   -4   14   -3   11    0    6    6    1  ...     383     392   \n",
       "2       12  -10   11    8   10   11   10    8   12   11  ...     198     198   \n",
       "3       12  114  168  167  171  166  172  166  172  172  ...     201     202   \n",
       "4       12  -13   11   11   10   12   17   16   20   21  ...    -365    -368   \n",
       "..     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...     ...   \n",
       "647     12   30   65   64   64   61   68   62   63   66  ...     118     122   \n",
       "648     21   80  119  120  118  110  106  106  105  104  ...     -94     -89   \n",
       "649     33   28   76   70   68   76   63   78   64   75  ...      79      77   \n",
       "650     12  -62  -58  -59  -57  -57  -57  -59  -55  -59  ...     193     194   \n",
       "651      2  -77  -73  -70  -68  -71  -71  -73  -72  -75  ...    -387    -381   \n",
       "\n",
       "     ry2040  ry2041  ry2042  ry2043  ry2044  ry2045  ry2046  ry2047  \n",
       "0       630     622     617     615     611     607     605     602  \n",
       "1       383     386     389     381     388     384     385     387  \n",
       "2       198     200     201     199     199     199     197     199  \n",
       "3       198     200     198     197     197     194     197     192  \n",
       "4      -369    -369    -366    -372    -371    -371    -372    -374  \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "647     119     119     117     117     118     114     119     117  \n",
       "648     -88     -85     -83     -84     -86     -88     -90     -88  \n",
       "649      79      77      76      82      78      82      77      86  \n",
       "650     191     192     189     187     188     186     186     185  \n",
       "651    -380    -378    -380    -380    -380    -381    -382    -383  \n",
       "\n",
       "[652 rows x 8193 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"../data/emvic/train.csv\"  # Path to train CSV\n",
    "test_file = \"../data/emvic/test.csv\"  # Path to test CSV\n",
    "label_column = \"class\"\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = list(df[\"class\"].unique())\n",
    "unique.sort()\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(class     0\n",
       " lx0       0\n",
       " lx1       0\n",
       " lx2       0\n",
       " lx3       0\n",
       "          ..\n",
       " ry2043    0\n",
       " ry2044    0\n",
       " ry2045    0\n",
       " ry2046    0\n",
       " ry2047    0\n",
       " Length: 8193, dtype: int64,\n",
       " 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_counts = df.isna().sum()\n",
    "na_counts, na_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(class     0\n",
       " lx0       0\n",
       " lx1       0\n",
       " lx2       0\n",
       " lx3       0\n",
       "          ..\n",
       " ry2043    0\n",
       " ry2044    0\n",
       " ry2045    0\n",
       " ry2046    0\n",
       " ry2047    0\n",
       " Length: 8193, dtype: int64,\n",
       " 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_counts = (df == '').sum()\n",
    "empty_counts, empty_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15164/4165127518.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df.replace('?', np.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels unique values: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37]\n",
      "Test labels unique values: [18]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset class\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Preprocess data with feature reduction (SelectKBest)\n",
    "# Preprocess data with feature reduction (SelectKBest)\n",
    "def preprocess_data(train_file, test_file, label_column, missing_threshold=0.2, top_k_features=15):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    # Handle column names with unexpected characters\n",
    "    train_df.columns = train_df.columns.str.strip().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    test_df.columns = test_df.columns.str.strip().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Replace missing values\n",
    "    train_df.replace('?', np.nan, inplace=True)\n",
    "    test_df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Handle columns with too many missing values\n",
    "    valid_columns = train_df.columns[train_df.isnull().mean() < missing_threshold]\n",
    "    if label_column not in valid_columns:\n",
    "        valid_columns = valid_columns.insert(0, label_column)  # Ensure label column is retained\n",
    "\n",
    "    train_df = train_df[valid_columns]\n",
    "    test_df = test_df[valid_columns]\n",
    "\n",
    "    # Fill missing values\n",
    "    train_df.fillna(train_df.mean(), inplace=True)\n",
    "    test_df.fillna(train_df.mean(), inplace=True)\n",
    "\n",
    "    # Extract labels and features\n",
    "    train_labels = train_df[label_column].values\n",
    "    test_labels = test_df[label_column].values\n",
    "\n",
    "    train_features = train_df.drop(columns=[label_column]).values\n",
    "    test_features = test_df.drop(columns=[label_column]).values\n",
    "\n",
    "    # Combine unique labels from train and test\n",
    "    unique_labels = sorted(np.unique(np.concatenate([train_labels, test_labels])))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Map labels to integers\n",
    "    train_labels = np.array([label_mapping[label] for label in train_labels])\n",
    "    test_labels = np.array([label_mapping[label] for label in test_labels])\n",
    "\n",
    "    # Ensure labels are within the correct range\n",
    "    validate_labels(train_labels, len(unique_labels))\n",
    "    validate_labels(test_labels, len(unique_labels))\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Feature selection using SelectKBest\n",
    "    feature_selector = SelectKBest(f_classif, k=top_k_features)\n",
    "    train_features = feature_selector.fit_transform(train_features, train_labels)\n",
    "    test_features = feature_selector.transform(test_features)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TabularModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)  # Output dimension should be the number of unique labels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = \"../data/emvic/train.csv\"  # Path to train CSV\n",
    "    test_file = \"../data/emvic/test.csv\"  # Path to test CSV\n",
    "    label_column = \"class\"\n",
    "\n",
    "    # Set device to CPU temporarily for debugging\n",
    "    device = torch.device(\"cpu\")\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Preprocess data\n",
    "    train_features, train_labels, test_features, test_labels = preprocess_data(train_file, test_file, label_column)\n",
    "\n",
    "    # Check if the labels are within the valid range\n",
    "    print(f\"Train labels unique values: {np.unique(train_labels)}\")\n",
    "    print(f\"Test labels unique values: {np.unique(test_labels)}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TabularDataset(train_features, train_labels)\n",
    "    test_dataset = TabularDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Define model, loss, and optimizer\n",
    "    input_dim = train_features.shape[1]  # Number of selected features\n",
    "    output_dim = len(np.unique(train_labels)) + 1 # Number of unique labels\n",
    "\n",
    "    model = TabularModel(input_dim, output_dim).to(device)  # Ensure the model is transferred to the correct device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 38, device(type='cpu'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim, output_dim, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37]\n",
      "Unique test labels: [18]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique train labels: {np.unique(train_labels)}\")\n",
    "print(f\"Unique test labels: {np.unique(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion, device)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[73], line 106\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    105\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 106\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/mnt/f/coding_workspace2/FL/Eye-Gaze-Authentication/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/f/coding_workspace2/FL/Eye-Gaze-Authentication/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/f/coding_workspace2/FL/Eye-Gaze-Authentication/venv/lib/python3.12/site-packages/torch/optim/adam.py:197\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/f/coding_workspace2/FL/Eye-Gaze-Authentication/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:430\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_compiling()\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    429\u001b[0m     ):\n\u001b[0;32m--> 430\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    433\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[1;32m    434\u001b[0m         ):\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    436\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    438\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m             )\n",
      "File \u001b[0;32m/mnt/f/coding_workspace2/FL/Eye-Gaze-Authentication/venv/lib/python3.12/site-packages/torch/cuda/graphs.py:30\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    #try:\n",
    "    if True:\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}\")\n",
    "    # except RuntimeError as e:\n",
    "    #     print(f\"Error during training: {e}\")\n",
    "    #     if \"CUDA error\" in str(e):\n",
    "    #         print(\"Attempting to run on CPU due to CUDA error.\")\n",
    "    #         device = torch.device(\"cpu\")\n",
    "    #         model = model.to(device)\n",
    "    #         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def preprocess_data(file_path, label_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    labels = df[label_column].values\n",
    "    features = df.drop(columns=[label_column])\n",
    "    \n",
    "    # Handle missing values (drop columns with too many missing values or impute)\n",
    "    valid_columns = features.columns[features.isnull().mean() < 0.2]  # Keep columns with <20% missing\n",
    "    features = features[valid_columns]\n",
    "    features = features.fillna(features.mean())  # Fill missing values with column mean\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TabularModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "def main():\n",
    "    # File paths and label column name\n",
    "    train_file = \"train.csv\"\n",
    "    test_file = \"test.csv\"\n",
    "    label_column = \"class\"  # Replace with the actual label column name\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_features, train_labels = preprocess_data(train_file, label_column)\n",
    "    test_features, test_labels = preprocess_data(test_file, label_column)\n",
    "    \n",
    "    # Create Datasets and DataLoaders\n",
    "    train_dataset = TabularDataset(train_features, train_labels)\n",
    "    test_dataset = TabularDataset(test_features, test_labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Model, loss, optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TabularModel(input_dim=train_features.shape[1], output_dim=len(np.unique(train_labels))).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 20\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
